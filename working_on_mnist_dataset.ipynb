{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "working on mnist dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohit-kapoor/Machine-learning/blob/master/working_on_mnist_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFZXlw2eLLuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "139b2a96-b8a3-4737-aa2b-c58e232a660f"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "print(len(x_train))\n",
        "print(len(x_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "60000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_SQUDCHLLuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "c20b0fca-fcff-40ed-f1e2-099fb12fa790"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "fig=plt.figure(figsize=(20,20))\n",
        "for i in range(6):\n",
        "    ax=fig.add_subplot(1,6,i+1,xticks=[],yticks=[])\n",
        "    ax.imshow(x_train[i],cmap='gray')\n",
        "    ax.set_title(str(y_train[i]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAADBCAYAAABIbSwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAADUgCEOAAAAQA0Y4gAAAADUwP8FPgp4o18akEgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x1440 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoSTTK4tLLu7",
        "colab_type": "text"
      },
      "source": [
        "#### now we will try to print one of the image in more detail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgTBc8_YLLu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1027
        },
        "outputId": "7a086674-efcc-4fc4-84d2-f1ae0f81ee60"
      },
      "source": [
        "def vizualize_input(img,ax):\n",
        "    ax.imshow(img,cmap='gray')\n",
        "    width,height=img.shape\n",
        "    thresh=img.max()/2.5\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            ax.annotate(str(round(img[x][y],2)),xy=(y,x),\n",
        "            horizontalalignment='centre',\n",
        "            verticalalignment='centre',\n",
        "            color='white' if img[x][y]<thresh else 'black')\n",
        "    \n",
        "fig=plt.figure(figsize=(12,12))\n",
        "ax=fig.add_subplot(1,1,1)\n",
        "vizualize_input(x_train[0],ax)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-53d2cf02a3f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvizualize_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-53d2cf02a3f5>\u001b[0m in \u001b[0;36mvizualize_input\u001b[0;34m(img, ax)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mhorizontalalignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'centre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mverticalalignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'centre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             color='white' if img[x][y]<thresh else 'black')\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, s, xy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdedent_interpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentityTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'clip_on'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, s, xy, xytext, xycoords, textcoords, arrowprops, annotation_clip, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxytext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0mText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrowprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrowprops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, text, color, verticalalignment, horizontalalignment, multialignment, fontproperties, rotation, linespacing, rotation_mode, usetex, wrap, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_usetex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musetex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verticalalignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverticalalignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_horizontalalignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizontalalignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multialignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultialignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mset_verticalalignment\u001b[0;34m(self, align)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malign\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlegal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             raise ValueError('Vertical alignment must be one of %s' %\n\u001b[0;32m-> 1174\u001b[0;31m                              str(legal))\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verticalalignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Vertical alignment must be one of ('top', 'bottom', 'center', 'baseline', 'center_baseline')"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArEAAAKvCAYAAAB9BpfGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgDQjogFAKCd/w9BaiTagHXahQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NsuPSHnLLvL",
        "colab_type": "text"
      },
      "source": [
        "#### now we will rescle the image by dividing every pixel in every image by 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPMcJRdDLLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=x_train.astype('float32')/255\n",
        "x_test=x_test.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KO-xGp0LLvV",
        "colab_type": "text"
      },
      "source": [
        "#### encode categorical integer labels using a one-hot-scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sWfTFOKLLvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "fe6ac91a-4147-47d9-a5ee-a759d9e24b6f"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "# print first ten (integer-valued ) training labels\n",
        "print('Integer-valued labels:')\n",
        "print(y_train[:10])\n",
        "# one hot encode the labels\n",
        "y_train=np_utils.to_categorical(y_train,10)\n",
        "y_test=np_utils.to_categorical(y_test,10)\n",
        "print('one-hot-labels:')\n",
        "print(y_train[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Integer-valued labels:\n",
            "[5 0 4 1 9 2 1 3 1 4]\n",
            "one-hot-labels:\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XejuNWU_LLvh",
        "colab_type": "text"
      },
      "source": [
        "#### define the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpzef04wLLvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "83933cb5-d638-433e-e592-ce59e89a9c1a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten\n",
        "\n",
        "# define the model\n",
        "model =Sequential()\n",
        "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 12:49:33.483589 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0615 12:49:33.518662 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0615 12:49:33.538716 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0615 12:49:33.555497 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0615 12:49:33.566007 140413135624064 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUgsCSeKLLvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "8cc4dedc-d589-4f8a-ab1d-83d7dbad5e30"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0615 12:49:41.268854 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0615 12:49:41.299893 140413135624064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtBsSBAVLLv7",
        "colab_type": "text"
      },
      "source": [
        "### calculate the accuracy on set before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTiuuc-7LLwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8543dfb-345b-4370-ab75-25b196a794c9"
      },
      "source": [
        "score=model.evaluate(x_test,y_test,verbose=0)\n",
        "accuracy=100*score[1]\n",
        "print(\"the accuracy is\",accuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy is 11.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewlQPEEhLLwR",
        "colab_type": "text"
      },
      "source": [
        "## now we will train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9uN3GDTLLwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6834
        },
        "outputId": "339dbb4b-41cf-49b3-e34f-16b5fa470e90"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer=ModelCheckpoint(filepath='mnist.model.best.hdf5',\n",
        "                             verbose=1,save_best_only=True)\n",
        "hist=model.fit(x_train,y_train,batch_size=128,epochs=100,\n",
        "              validation_split=0.2,callbacks=[checkpointer],\n",
        "              verbose=1,shuffle=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0264 - acc: 0.9925 - val_loss: 0.1046 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.10465, saving model to mnist.model.best.hdf5\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.1116 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.10465\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.1207 - val_acc: 0.9787\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.10465\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.1183 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.10465\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0191 - acc: 0.9948 - val_loss: 0.1213 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.10465\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0188 - acc: 0.9947 - val_loss: 0.1083 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.10465\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.1267 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.10465\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.1369 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.10465\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.1305 - val_acc: 0.9802\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.10465\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0174 - acc: 0.9955 - val_loss: 0.1347 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.10465\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0143 - acc: 0.9962 - val_loss: 0.1430 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.10465\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.1681 - val_acc: 0.9774\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.10465\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0153 - acc: 0.9958 - val_loss: 0.1392 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.10465\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0150 - acc: 0.9964 - val_loss: 0.1423 - val_acc: 0.9803\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.10465\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0127 - acc: 0.9966 - val_loss: 0.1403 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.10465\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0135 - acc: 0.9968 - val_loss: 0.1367 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.10465\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0150 - acc: 0.9962 - val_loss: 0.1389 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.10465\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0126 - acc: 0.9964 - val_loss: 0.1462 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.10465\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.1439 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.10465\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.1467 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.10465\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0124 - acc: 0.9972 - val_loss: 0.1563 - val_acc: 0.9806\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.10465\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0119 - acc: 0.9971 - val_loss: 0.1575 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.10465\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0110 - acc: 0.9972 - val_loss: 0.1516 - val_acc: 0.9809\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.10465\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.1463 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.10465\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0127 - acc: 0.9972 - val_loss: 0.1496 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.10465\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0124 - acc: 0.9971 - val_loss: 0.1518 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.10465\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0126 - acc: 0.9974 - val_loss: 0.1435 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.10465\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.1465 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.10465\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.1606 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.10465\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0110 - acc: 0.9974 - val_loss: 0.1658 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.10465\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.1500 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.10465\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0107 - acc: 0.9974 - val_loss: 0.1560 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.10465\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.1621 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.10465\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.1630 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.10465\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0087 - acc: 0.9980 - val_loss: 0.1553 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.10465\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0092 - acc: 0.9980 - val_loss: 0.1666 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.10465\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0096 - acc: 0.9979 - val_loss: 0.1663 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.10465\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0108 - acc: 0.9979 - val_loss: 0.1716 - val_acc: 0.9805\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.10465\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.1665 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.10465\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0122 - acc: 0.9977 - val_loss: 0.1672 - val_acc: 0.9811\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.10465\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0094 - acc: 0.9980 - val_loss: 0.1555 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.10465\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.1624 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.10465\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.1641 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.10465\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0111 - acc: 0.9980 - val_loss: 0.1769 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.10465\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.1627 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.10465\n",
            "Epoch 46/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.1635 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.10465\n",
            "Epoch 47/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 0.1679 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.10465\n",
            "Epoch 48/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.1817 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.10465\n",
            "Epoch 49/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0118 - acc: 0.9977 - val_loss: 0.1487 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.10465\n",
            "Epoch 50/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0084 - acc: 0.9983 - val_loss: 0.1682 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.10465\n",
            "Epoch 51/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0108 - acc: 0.9981 - val_loss: 0.1809 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.10465\n",
            "Epoch 52/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.1594 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.10465\n",
            "Epoch 53/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0092 - acc: 0.9982 - val_loss: 0.1631 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.10465\n",
            "Epoch 54/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0071 - acc: 0.9987 - val_loss: 0.1832 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.10465\n",
            "Epoch 55/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0087 - acc: 0.9985 - val_loss: 0.1667 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.10465\n",
            "Epoch 56/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0115 - acc: 0.9980 - val_loss: 0.1616 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.10465\n",
            "Epoch 57/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0110 - acc: 0.9982 - val_loss: 0.1612 - val_acc: 0.9838\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.10465\n",
            "Epoch 58/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0104 - acc: 0.9982 - val_loss: 0.1680 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.10465\n",
            "Epoch 59/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0103 - acc: 0.9981 - val_loss: 0.1681 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.10465\n",
            "Epoch 60/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0105 - acc: 0.9980 - val_loss: 0.1626 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.10465\n",
            "Epoch 61/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0085 - acc: 0.9983 - val_loss: 0.1652 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.10465\n",
            "Epoch 62/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0098 - acc: 0.9981 - val_loss: 0.1711 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.10465\n",
            "Epoch 63/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0081 - acc: 0.9984 - val_loss: 0.1750 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.10465\n",
            "Epoch 64/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0090 - acc: 0.9984 - val_loss: 0.1746 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.10465\n",
            "Epoch 65/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0079 - acc: 0.9986 - val_loss: 0.1579 - val_acc: 0.9837\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.10465\n",
            "Epoch 66/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0081 - acc: 0.9985 - val_loss: 0.1674 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.10465\n",
            "Epoch 67/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0113 - acc: 0.9981 - val_loss: 0.1736 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.10465\n",
            "Epoch 68/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0079 - acc: 0.9983 - val_loss: 0.1772 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.10465\n",
            "Epoch 69/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.1690 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.10465\n",
            "Epoch 70/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0062 - acc: 0.9989 - val_loss: 0.1531 - val_acc: 0.9841\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.10465\n",
            "Epoch 71/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0095 - acc: 0.9985 - val_loss: 0.1656 - val_acc: 0.9837\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.10465\n",
            "Epoch 72/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0083 - acc: 0.9984 - val_loss: 0.1677 - val_acc: 0.9838\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.10465\n",
            "Epoch 73/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0085 - acc: 0.9986 - val_loss: 0.1605 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.10465\n",
            "Epoch 74/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0081 - acc: 0.9986 - val_loss: 0.1708 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.10465\n",
            "Epoch 75/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0057 - acc: 0.9990 - val_loss: 0.1695 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.10465\n",
            "Epoch 76/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0063 - acc: 0.9989 - val_loss: 0.1780 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.10465\n",
            "Epoch 77/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 0.1756 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.10465\n",
            "Epoch 78/100\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0061 - acc: 0.9990 - val_loss: 0.1645 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.10465\n",
            "Epoch 79/100\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0066 - acc: 0.9989 - val_loss: 0.1712 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.10465\n",
            "Epoch 80/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0092 - acc: 0.9985 - val_loss: 0.1678 - val_acc: 0.9836\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.10465\n",
            "Epoch 81/100\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0105 - acc: 0.9984 - val_loss: 0.1769 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.10465\n",
            "Epoch 82/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1719 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.10465\n",
            "Epoch 83/100\n",
            "48000/48000 [==============================] - 2s 34us/step - loss: 0.0080 - acc: 0.9986 - val_loss: 0.1694 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.10465\n",
            "Epoch 84/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0096 - acc: 0.9985 - val_loss: 0.1796 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.10465\n",
            "Epoch 85/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1682 - val_acc: 0.9835\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.10465\n",
            "Epoch 86/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0052 - acc: 0.9989 - val_loss: 0.1753 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.10465\n",
            "Epoch 87/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0057 - acc: 0.9987 - val_loss: 0.1729 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.10465\n",
            "Epoch 88/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.1830 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.10465\n",
            "Epoch 89/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0077 - acc: 0.9986 - val_loss: 0.1739 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.10465\n",
            "Epoch 90/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1710 - val_acc: 0.9840\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.10465\n",
            "Epoch 91/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0086 - acc: 0.9986 - val_loss: 0.1860 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.10465\n",
            "Epoch 92/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.1849 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.10465\n",
            "Epoch 93/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.2013 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.10465\n",
            "Epoch 94/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0081 - acc: 0.9987 - val_loss: 0.1870 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.10465\n",
            "Epoch 95/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.1920 - val_acc: 0.9831\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.10465\n",
            "Epoch 96/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0069 - acc: 0.9990 - val_loss: 0.1960 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.10465\n",
            "Epoch 97/100\n",
            "48000/48000 [==============================] - 2s 33us/step - loss: 0.0068 - acc: 0.9989 - val_loss: 0.1833 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.10465\n",
            "Epoch 98/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0065 - acc: 0.9989 - val_loss: 0.1746 - val_acc: 0.9837\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.10465\n",
            "Epoch 99/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 0.1777 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.10465\n",
            "Epoch 100/100\n",
            "48000/48000 [==============================] - 2s 32us/step - loss: 0.0082 - acc: 0.9988 - val_loss: 0.1840 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.10465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp0Zzxp0LLwg",
        "colab_type": "text"
      },
      "source": [
        "## load the model with the best classification accuracy on the set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgKHiAfLLwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the models which yielded the best validaion accuracy on the set\n",
        "model.load_weights('mnist.model.best.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ31GnMNLLwq",
        "colab_type": "text"
      },
      "source": [
        "## calculate the classification accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KKkorulLLwt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4ee7314e-2c87-48d0-fdf2-d2727c9c63cf"
      },
      "source": [
        "score=model.evaluate(x_test,y_test,verbose=0)\n",
        "print(score)\n",
        "accuracy=100*score[1]\n",
        "print(accuracy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08809179703994396, 0.9829]\n",
            "98.29\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}